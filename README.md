![logo_ironhack_blue 7](https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png)

# End to End Data Science Project

## Introduction

Welcome to your final project! The goal of this project is for you to apply your data science skills that you learned throughout the bootcamp by completing an end-to-end analysis on a real-world dataset. The project will also assist you in demonstrating your proficiency in the skills required for your desired career path and developing your portfolio to showcase your abilities to potential recruiters.

If you're struggling to come up with a topic, we have provided a list of datasets for you to consider. However, we highly recommend that you explore and select a topic and dataset that personally interests you, as this will make the project more engaging and rewarding.

## Setup

This is an individual project.

## Project Brief

The goal is to build, on top of a business case, a predictive model in Python.

This project will require you to select a business case of your choice and iterate through the whole data science process, by doing data collection, data cleaning and wrangling, exploratory data analysis, feature engineering, preprocessing, model selection, evaluation and data visualization. The project should be structured as a complete pipeline that includes every step of the process.


## Prerequisites

In order to successfully complete the final project, you should possess a strong understanding of several key concepts, including Python programming, data wrangling, and exploratory data analysis (EDA), as required for previous projects. In addition to these, the following are essential prerequisites that you should have before beginning this final project:


### Minimum prerequisites:

- Machine Learning Key Concepts: supervised and unsupervised learning, classification and regression, bias-variance tradeoff, train-test-cross-validation, feature engineering (including encoding, scaling, and selection), model evaluation, dealing with unbalanced datasets, and hyperparameter tuning.
- Machine Learning Supervised Models such as Linear Regression, Logistic Regression, Decision Trees, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Naive Bayes, and Ensemble Models.
- Unsupervised Models, such as KMeans, Hierarchical Clustering and PCA.
- Strong understanding of the most appropriate metrics and preprocessing techniques based on the specific problem and context.
- Experience with model enhancement iteration.

### Nice to have prerequisites:

- Time Series
- NLP
- Deep Learning


## Project week

This project will last the whole week. Presentations will be held on the last day of the course (please ask your teaching team about the presentation schedule).
Also, remember that by the end of the bootcamp, you should have submitted ALL your labs (and projects).

<br>

![presentations](https://img.freepik.com/premium-vector/business-people-working-office-character_48866-1140.jpg)

<br>

## Getting Started

- Project Planning:
    - Create a Kanban or Trello Board for progress tracking and objective management.
    - Create a repository for your project and commit and push frequently
- Problem Selection and Data Gathering:
    - Select a business problem to address.
    - Locate and gather necessary data.
    - Explore and understand each field in the dataset.
- Data Analysis:
    - Apply statistical techniques and data visualization to examine feature relationships.
    - Make informed guesses about features requiring further investigation.
- Data Cleaning and Manipulation:
    - Handle outliers and missing values.
    - Perform type casting and feature selection.
    - Convert categorical data to numerical.
    - Use statistical methods for data analysis.
- Machine Learning:
    - Apply machine learning on the objective variable data for prediction, classification, clustering, etc.
    - Experiment with different models and hyperparameters.
    - Choose the simplest effective model.
    - Clearly define the metric for evaluating "best" results.
- Presentation of Findings:
    - Present a statistical summary and data visualizations.
    
## Project artifact:

### Mandatory:

1. **Dataset Integration:**
   - Use at least two different data sources.
     - For example, combine data from two tables and join them with web scraped data.
   
2. **Comprehensive Documentation:**
   - A detailed README file explaining:
     - The problem statement and business case.
     - Instructions on how to run the code and reproduce the results.
     - Any assumptions made during the project.
     - ChatGPT can be a big help here
     
3. **Data Cleaning and Preparation:**
   - Clear handling of outliers and missing values.
   - Proper type casting and conversion of categorical data to numerical.
   
4. **Exploratory Data Analysis (EDA):**
   - .py file responsible for the ETL part of the processing of the dataset
   - Tableau Dashboard containing an evaluation and overview of the dataset

5. **Model Building and Evaluation:**
   - Application of at least three different machine learning models.
   - Clear explanation of model selection and evaluation metrics.
   - Hyperparameter tuning and model performance comparison.
   - .py file containing the experiments done on the several models

6. **Final Model:**
   - Selection and justification of the best model based on the evaluation metrics.
   - Discussion of the model's performance, including strengths and limitations.
   
7. **Visualizations and Reporting:**
   - Visualizations that clearly communicate findings and insights.
   - Tableau Dashboard presenting the findings from your experiments

8. **Product**
   - .py file to run the product. It should answer que question: "What if I wanted to **use** what I just this in a real life scenario?"

## Highly Recommended Artifact:

1. **User-Friendly Product:**
   - Transform the project into a usable product.
     - For example, a web app where users can input new data and get predictions.
   
2. **Advanced Data Processing:**
   - Implementation of more sophisticated data cleaning techniques.
     - For instance, using machine learning algorithms to handle missing or noisy data.
  
3. **Refresh SQL knowledge:**
   - Inject your processed dataset into an SQL database and have your Model training steps read from there 

## Good Add-Ons:
   
1. **Feature Engineering:**
   - Consider Feature Engineering tecniques like normalization or standardization (if applicable)
   - Use advanced feature engineering techniques like PCA for dimensionality reduction.

2. **Ensemble Methods:**
   - Use of stacking techniques or other ensemble methods to improve predictions.

3. **Include a GenAI element in the project:**:
   - Leverage GenAI in your application (user experience, data cleaning, etc.) 

4. **Run your product in a server**:
   - Deploy your product to a cloud service to be used by the entire class
  
5. **Code Quality:**
   - Well-commented and structured code.
   - Use of modular coding practices.

## Bonus Details:

 - **ML Experience:**
   - You MLFlow (locally) to track the several experiences you will so
   
 - **Unexpected Data Sources:**
   - Use unconventional data sources such as tweets, Reddit comments, or even data from IoT devices.
   
 - **Live Data Update:**
   - Create a feature where your project can update with live data. This could be a live feed of social media mentions, stock prices, or weather data.

 - **Data Visualization Challenges:**
   - Create a challenging and unusual data visualization, like a 3D graph or an interactive dashboard with unexpected but insightful data stories.

 - **API Integration:**
   - Build an API that allows external applications to interact with your model, providing predictions or data insights via API calls.

 - **Voice Assistant Integration:**
   - Integrate your model with a voice assistant like Amazon Alexa or Google Assistant to provide voice-activated insights.


## Deliverables

You must submit the following deliverables in order for the project to be deemed complete:

- A new repo on your Github account.
    - Working code that meets all technical requirements, built by you.
        - Python, SQL files, Tableau/PowerBI report, or any additional needed files for your work (you might use Jupyter Notebooks because it is easier, but these should not exist in the final version of the product)
    - A README with the completed project documentation.
    - The URL of the Tableau for your project presentation.
- **Presentation:** when presenting your work, there are many important factors to consider, such as the content of your presentation and the way you deliver it.
- Paste your own repo's link in the Student Portal Project Activity


## Presentations

You are free to present your final project in the way that you feel best represents your work. But it is important you communicate a compelling story about how you set about the project, the techniques used, insights gained and key learnings.